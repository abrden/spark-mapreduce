{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2015-02 2do Recuperatorio\n",
    "Se tiene un RDD con libros en donde cada registro es un texto. Se pide obtener todos los anagramas de mas de 7 letras que puedan encontrarse. El formato de salida debe ser una lista de listas en donde cada lista tiene un conjunto de palabras que son anagramas. Ejemplo: [[discounter,introduces,reductions],[percussion,supersonic]...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Lorem ascent ipsum dolor sit amet, autem melius ea pri, pro malis theophrastus ut, in duo vide vocibus instructior. Duo denique torquatos dissentiunt ei, cu reprimique interesset mei. Duo nulla paulo ea. Ex commodo convenire gubergren mea, verterem lobortis ut mea. Qui debet ocurreret hendrerit ea, iracundia rationibus cotidieque et sea. recontrarepetida recontrarepetida recontrarepetida recontrarepetida',\n",
       " u'Ius secant nibh utinam ut. Vel mollis integre labores an. Qualisque incorrupte mea ut, labitur sententiae theophrastus est ea. Usu in alterum rationibus, vix dico prodesset ex, suas offendit postulant  recontrarepetida recontrarepetida recontrarepetida ne eum. Has causae corrumpit te.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile = sc.textFile(\"data/texts.txt\")\n",
    "textFile.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Lorem', u'ascent', u'ipsum', u'dolor', u'sit']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = textFile.flatMap(lambda line: line.split())\n",
    "words.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'doctorwho', u'torchwood']]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.filter(lambda x: len(x) > 7)\\\n",
    "     .distinct()\\\n",
    "     .map(lambda x:(''.join(sorted(x)), [x]))\\\n",
    "     .reduceByKey(lambda a, b: a + b)\\\n",
    "     .filter(lambda x: len(x[1]) > 1)\\\n",
    "     .map(lambda x: x[1])\\\n",
    "     .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2015-01 1er Parcial\n",
    "Dada una colección de documentos queremos encontrar frases de 1 , 2 o 3 palabras que sean anagramas de otras. Por ejemplo: (“Postmaster”, “Stamp store”) o (“A telescope” , “To see Place”) o (“The cockroach”, “cook catch her”). Esta tarea implica una combinatoria muy difícil por lo que se decide usar Map-Reduce para paralelizarla. Usando Map-Reduce programar la solución a este problema listando todos los pares de anagramas entre frases de 1, 2 o 3 palabras. Como puede verse en los ejemplos se ignoran mayúsculas y minúsculas y los espacios en blanco, puntuación, etc. Suponer que existe la función word_tokenizer que recibe un texto y devuelve un vector de palabras ya convertidas a minúsculas y sin puntuación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenizer(str, pipeline):\n",
    "    if len(pipeline) == 0:\n",
    "        return str\n",
    "    else:\n",
    "        return tokenizer(pipeline[0](str), pipeline[1:])\n",
    "    \n",
    "def word_tokenizer(doc):\n",
    "    pipeline = [ lambda s: re.sub('[^\\w\\s]', '', s), \n",
    "            lambda s: re.sub('[\\d]', '', s),\n",
    "            lambda s: s.lower() ]\n",
    "    \n",
    "    return tokenizer(doc, pipeline).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def biphrases(t):\n",
    "    return [ ' '.join(t[i:i + 2]) for i in range(0, len(t) - 1)]\n",
    "\n",
    "def triphrases(t):\n",
    "    return [ ' '.join(t[i:i + 3]) for i in range(0, len(t) - 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'ascent', u'secant', u'stance'),\n",
       " (u'mea', u'eam'),\n",
       " (u'postmaster', u'stamp store'),\n",
       " (u'cu ei', u'ei cu'),\n",
       " (u'doctorwho', u'torchwood'),\n",
       " (u'ut mea', u'mea ut', u'autem'),\n",
       " (u'te', u'et'),\n",
       " (u'eum in', u'in eum'),\n",
       " (u'et sea', u'est ea')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases = sc.textFile(\"data/texts.txt\")\\\n",
    "             .map(lambda x: word_tokenizer(x))\\\n",
    "             .flatMap(lambda x: triphrases(x) + biphrases(x) + x)\\\n",
    "             .distinct()\\\n",
    "             .map(lambda x: (''.join(sorted(''.join(x).replace(' ',''))), [x]))\\\n",
    "             .reduceByKey(lambda a, b: a + b)\\\n",
    "             .filter(lambda x: len(x[1]) > 1)\\\n",
    "             .map(lambda x: tuple(x[1]))\n",
    "phrases.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
